{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleaned_data.csv')\n",
    "languages = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mist durchschaut egal mach einfach</td>\n",
       "      <td>sénat bernard arnault dépeint bienfaiteur presse</td>\n",
       "      <td>pregare insieme uni altri darci fare insieme c...</td>\n",
       "      <td>pandemia silenciosa infecciones bacterias resi...</td>\n",
       "      <td>psychologist robert coplan studies concept alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh süße watschelente</td>\n",
       "      <td>récap politique jour</td>\n",
       "      <td>popolazioni isole tonga colpite giorni scorsi ...</td>\n",
       "      <td>creo sesgo masculino despreciar quejas femenin...</td>\n",
       "      <td>things like love trust caring simply work unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ex otage ingrid betancourt précandidate présid...</td>\n",
       "      <td>oggi coloro carcere tenerezza dio raggiunga ca...</td>\n",
       "      <td>fiscal peruana anunci comenzar investigaci n p...</td>\n",
       "      <td>temperature kinds factors shape views decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stups doof</td>\n",
       "      <td>exil france kamal mouzawak partage saveurs liban</td>\n",
       "      <td>tenerezza questione emotiva sentimentale esper...</td>\n",
       "      <td>temores mosc est tratando dividir desestabiliz...</td>\n",
       "      <td>days warmer average people would give money co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stupst</td>\n",
       "      <td>egypte capture opposant islamiste après atterr...</td>\n",
       "      <td>magi venuti oriente betlemme onorare re messia...</td>\n",
       "      <td>putin amenaz apropiadas medidas técnico milita...</td>\n",
       "      <td>although people quite aware global warming bel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               German  \\\n",
       "0  mist durchschaut egal mach einfach   \n",
       "1                oh süße watschelente   \n",
       "2                                 NaN   \n",
       "3                          stups doof   \n",
       "4                              stupst   \n",
       "\n",
       "                                              French  \\\n",
       "0   sénat bernard arnault dépeint bienfaiteur presse   \n",
       "1                               récap politique jour   \n",
       "2  ex otage ingrid betancourt précandidate présid...   \n",
       "3   exil france kamal mouzawak partage saveurs liban   \n",
       "4  egypte capture opposant islamiste après atterr...   \n",
       "\n",
       "                                             Italian  \\\n",
       "0  pregare insieme uni altri darci fare insieme c...   \n",
       "1  popolazioni isole tonga colpite giorni scorsi ...   \n",
       "2  oggi coloro carcere tenerezza dio raggiunga ca...   \n",
       "3  tenerezza questione emotiva sentimentale esper...   \n",
       "4  magi venuti oriente betlemme onorare re messia...   \n",
       "\n",
       "                                             Spanish  \\\n",
       "0  pandemia silenciosa infecciones bacterias resi...   \n",
       "1  creo sesgo masculino despreciar quejas femenin...   \n",
       "2  fiscal peruana anunci comenzar investigaci n p...   \n",
       "3  temores mosc est tratando dividir desestabiliz...   \n",
       "4  putin amenaz apropiadas medidas técnico milita...   \n",
       "\n",
       "                                             English  \n",
       "0  psychologist robert coplan studies concept alo...  \n",
       "1  things like love trust caring simply work unde...  \n",
       "2  temperature kinds factors shape views decision...  \n",
       "3  days warmer average people would give money co...  \n",
       "4  although people quite aware global warming bel...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_rows(df, language):\n",
    "    series = df[language]\n",
    "\n",
    "    # Get the index of any empty rows\n",
    "    empty_rows = df[(df[language] == '') | (df[language].isna())].index\n",
    "    \n",
    "    # Drop those rows using the index\n",
    "    empty_removed = series.drop(empty_rows, axis=0)\n",
    "\n",
    "    # Reset the index\n",
    "    empty_removed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return empty_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_paths = {\n",
    "    'German' : './wordclouds/german_wordcloud.png',\n",
    "    'French' : './wordclouds/French_wordcloud.png',\n",
    "    'Italian': './wordclouds/Italian_wordcloud.png',\n",
    "    'Spanish': './wordclouds/Spanish_wordcloud.png',\n",
    "    'English': './wordclouds/English_wordcloud.png'\n",
    "}\n",
    "\n",
    "for language in languages:\n",
    "    # Drop the empty rows\n",
    "    target_dropped = remove_empty_rows(df, language)\n",
    "\n",
    "    # Combine all the text into a single string\n",
    "    long_string = \" \".join(target_dropped)\n",
    "\n",
    "    # Initialize the wordcloud\n",
    "    wc = WordCloud(background_color='white', contour_color='steelblue', height=600, width=800)\n",
    "\n",
    "    # Add data to the wordcloud\n",
    "    wc.generate(long_string)\n",
    "\n",
    "    # Save the cloud as a png file\n",
    "    wc.to_file(wordcloud_paths[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize wordcloud\n",
    "# plt.imshow(wc, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    # Drop the empty rows\n",
    "    target_dropped = remove_empty_rows(df, language)\n",
    "\n",
    "    # Initialize counter vectorizer\n",
    "    vec = CountVectorizer()\n",
    "\n",
    "    # Transform to document term matrix\n",
    "    X = vec.fit_transform(target_dropped)\n",
    "\n",
    "    # Create DataFrame\n",
    "    tdm = pd.DataFrame(data=X.toarray(), columns=vec.get_feature_names_out())\n",
    "\n",
    "    # Sort DataFrame by word frequency\n",
    "    words = tdm.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    words.to_csv(f'./data/common_words/{language}_common.csv', header=['count'], index_label='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c13c2d661185228a4c4d63993c20c3e5bdf2ba4e265046529d225752df3d5e17"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
